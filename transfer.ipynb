{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '''\n",
    "A revolutionary way for guiding the eyes through text using artificial fixation spots to make reading easier. As a result, the reader's attention is drawn solely to the highlighted starting letters, leaving the word to be completed by the brain center. Bionic Reading attempts to foster greater in-depth reading and understanding of textual content in a digital environment dominated by shallow kinds of reading.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "帮我对s这个句子进行bionic reading处理\n",
    "所谓bionic reading就是将一个单词的前面一半字母加粗，你需要借用一个PDF库函数，将这句话导出成一个PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from reportlab.lib.pagesizes import A5\n",
    "from reportlab.lib.units import inch\n",
    "from reportlab.pdfgen import canvas\n",
    "\n",
    "# 加载英文模型\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def is_english(text):\n",
    "    doc = nlp(text)\n",
    "    return all(token.lang_ == 'en' for token in doc)\n",
    "\n",
    "def process_text_to_pdf(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    if not is_english(text):\n",
    "        raise ValueError(\"The text is not in English.\")\n",
    "\n",
    "    # 创建PDF\n",
    "    c = canvas.Canvas(output_file, pagesize=A5)\n",
    "    width, height = A5\n",
    "    y_position = height - 1 * inch  # 开始位置\n",
    "    x_position = 0.5 * inch  # X位置\n",
    "    c.setFont(\"Helvetica\", 12)\n",
    "\n",
    "    # 句子拆分\n",
    "    doc = nlp(text)\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        line = \"\"\n",
    "        for token in sent:\n",
    "            if token.text.lower() in ['a', 'of', 'the', 'and', 'in', 'to']:  # 助词\n",
    "                line += token.text + ' '\n",
    "            else:\n",
    "                bold_part = token.text[:len(token.text)//2]\n",
    "                normal_part = token.text[len(token.text)//2:]\n",
    "                line += bold_part + ' ' + normal_part + ' '  # 加粗部分和普通部分\n",
    "\n",
    "        # 自动换行处理\n",
    "        words = line.split()\n",
    "        for word in words:\n",
    "            if word.strip():  # 跳过空白\n",
    "                word_width = c.stringWidth(word, \"Helvetica\", 12)\n",
    "                if x_position + word_width > width - 0.5 * inch:  # 检查是否超出边界\n",
    "                    y_position -= 14  # 换行\n",
    "                    x_position = 0.5 * inch  # 重置X位置\n",
    "\n",
    "                # 判断是否需要加粗\n",
    "                if word.lower() not in ['a', 'of', 'the', 'and', 'in', 'to']:\n",
    "                    c.setFont(\"Helvetica-Bold\", 12)  # 加粗\n",
    "                else:\n",
    "                    c.setFont(\"Helvetica\", 12)  # 普通\n",
    "\n",
    "                c.drawString(x_position, y_position, word)  # 输出当前单词\n",
    "                x_position += word_width + 2  # 调整位置\n",
    "\n",
    "        y_position -= 14  # 行间距\n",
    "        if y_position < inch:  # 换页\n",
    "            c.showPage()\n",
    "            c.setFont(\"Helvetica\", 12)\n",
    "            y_position = height - 1 * inch\n",
    "            x_position = 0.5 * inch  # 重置X位置\n",
    "\n",
    "        y_position -= 14  # 在句子之间换行\n",
    "\n",
    "    c.save()\n",
    "\n",
    "# 使用示例\n",
    "input_txt = 'test.txt'\n",
    "output_pdf = 'output.pdf'\n",
    "process_text_to_pdf(input_txt, output_pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from docx import Document\n",
    "from docx.shared import Pt\n",
    "\n",
    "# 加载英文模型\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def is_english(text):\n",
    "    doc = nlp(text)\n",
    "    return all(token.lang_ == 'en' for token in doc)\n",
    "\n",
    "def process_text_to_word(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    if not is_english(text):\n",
    "        raise ValueError(\"The text is not in English.\")\n",
    "\n",
    "    # 创建Word文档\n",
    "    doc = Document()\n",
    "\n",
    "    # 句子拆分\n",
    "    doc_nlp = nlp(text)\n",
    "\n",
    "    for sent in doc_nlp.sents:\n",
    "        paragraph = doc.add_paragraph()\n",
    "        for token in sent:\n",
    "            if token.text.lower() in ['a', 'of', 'the', 'and', 'in', 'to']:  # 助词\n",
    "                paragraph.add_run(token.text + ' ')\n",
    "            else:\n",
    "                bold_part = token.text[:len(token.text)//2]\n",
    "                normal_part = token.text[len(token.text)//2:]\n",
    "\n",
    "                # 添加前半部分加粗\n",
    "                run = paragraph.add_run(bold_part)\n",
    "                run.bold = True\n",
    "\n",
    "                # 添加后半部分普通\n",
    "                paragraph.add_run(normal_part + ' ')\n",
    "\n",
    "        # 在句子之间换行\n",
    "        paragraph.add_run()  # 添加一个空段落\n",
    "\n",
    "    doc.save(output_file)\n",
    "\n",
    "# 使用示例\n",
    "input_txt = 'test.txt'\n",
    "output_docx = 'output.docx'\n",
    "process_text_to_word(input_txt, output_docx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from docx import Document\n",
    "from docx.shared import Pt, RGBColor\n",
    "\n",
    "# 加载英文模型\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def is_english(text):\n",
    "    doc = nlp(text)\n",
    "    return all(token.lang_ == 'en' for token in doc)\n",
    "\n",
    "def process_text_to_word(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    if not is_english(text):\n",
    "        raise ValueError(\"The text is not in English.\")\n",
    "\n",
    "    # 创建Word文档\n",
    "    doc = Document()\n",
    "\n",
    "    # 句子拆分\n",
    "    doc_nlp = nlp(text)\n",
    "\n",
    "    for sent in doc_nlp.sents:\n",
    "        paragraph = doc.add_paragraph()\n",
    "        for token in sent:\n",
    "            if token.text.lower() in ['a', 'of', 'the', 'and', 'in', 'to']:  # 助词\n",
    "                run = paragraph.add_run(token.text + ' ')\n",
    "                run.font.color.rgb = RGBColor(96, 96, 96)  # 灰色\n",
    "            else:\n",
    "                bold_part = token.text[:len(token.text)//2]\n",
    "                normal_part = token.text[len(token.text)//2:]\n",
    "\n",
    "                # 添加前半部分加粗\n",
    "                run = paragraph.add_run(bold_part)\n",
    "                run.bold = True\n",
    "\n",
    "                # 添加后半部分普通并设为灰色\n",
    "                run = paragraph.add_run(normal_part + ' ')\n",
    "                run.font.color.rgb = RGBColor(96, 96, 96)  # 灰色\n",
    "\n",
    "        # 在句子之间换行\n",
    "        paragraph.add_run()  # 添加一个空段落\n",
    "\n",
    "    doc.save(output_file)\n",
    "\n",
    "# 使用示例\n",
    "input_txt = 'test.txt'\n",
    "output_docx = 'output.docx'\n",
    "process_text_to_word(input_txt, output_docx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
